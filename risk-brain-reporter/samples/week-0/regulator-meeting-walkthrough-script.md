# Risk Brain – Regulator Pre-Engagement Walkthrough Script (v1.0)

**Meeting Objective (say this upfront):**

*"We are here before any AI execution is contemplated to validate that our shadow-only architecture, governance model, and evidentiary controls meet supervisory expectations in advance, not after the fact."*

---

## Slide 1 — Opening Positioning (30–45 seconds)

**Say:**

*"We operate a three-layer system.  
A is the executing core ledger.  
B is the Risk Brain — an AI intelligence layer.  
C is the Risk Brain Reporter — a governance artefact generator.*

*Today we are only discussing B and C, both of which are mechanically non-executing."*

**Pause. Then say clearly:**

*"No AI system in our environment currently has, or can obtain, authority to move funds, post to a ledger, adjust credit, or trigger treasury actions."*

---

## Slide 2 — Why We Came to You Early (30 seconds)

**Say:**

*"Most institutions deploy automation first and then back-fill controls.*

*We have inverted that. We have built:*

- *Mechanical non-execution*
- *Immutable governance evidence*
- *Weekly board-grade AI risk reporting*

*Before any execution rights are even requested."*

---

## Slide 3 — What the Risk Brain Actually Does (1 minute)

**Say:**

*"The Risk Brain runs four shadow domains in parallel:  
Payments routing optimisation, Fraud detection, AML behavioural detection, and Treasury liquidity stress forecasting.*

*All outputs are advisory only.  
Every output is policy-gated.  
Every output is recorded, versioned, and replayable."*

**Then say:**

*"At present, it is not even technically possible for these outputs to reach an execution surface."*

---

## Slide 4 — Formal Non-Execution Proof (Critical Slide – 1 minute)

**Point to the invariant table and say verbatim:**

*"During our Week-0 dry run:  
AI-origin execution violations: zero.  
Schema violations: zero.  
Policy-origin violations: zero."*

**Then state the legal sentence:**

*"At no point during this period was any AI-originated instruction capable of triggering ledger, payment, credit, treasury, or settlement execution."*

**Then pause and add:**

*"This is mechanically enforced through IAM, network policy, CI enforcement, and runtime isolation — not staff procedures."*

---

## Slide 5 — Evidence Chain & Immutability (45 seconds)

**Say:**

*"All governance artefacts are auto-generated.  
They are write-once, object-locked, hash-sealed, and versioned.*

*We operate under the assumption that future disputes will be evidentiary, not explanatory."*

**Then:**

*"We cannot retroactively 'tidy' AI governance even if we wanted to."*

---

## Slide 6 — Domain Outcomes (Keep Factual, 60 seconds)

**Say:**

*"For Week-0 at network level:  
Payments RL analysed ~72% of transactions with a net positive optimisation signal.  
Fraud produced 37 high-risk flags with 4 confirmed cases.  
AML surfaced 7 high-risk behaviours and one SMR.  
Treasury projected zero liquidity stress beyond current buffers."*

**Then:**

*"None of these signals were permitted to auto-execute."*

---

## Slide 7 — How This Aligns to Each Regulator (Targeted framing, 90 seconds)

### For APRA:

*"From our perspective this is operational risk containment for AI.  
The AI perimeter sits outside the execution perimeter.  
The board receives weekly, machine-generated AI governance.  
This keeps model risk observable without introducing prudential fragility."*

### For AUSTRAC:

*"The AML engine is behavioural and shadow-only.  
It enhances detection quality without altering SMR obligations or reporting authority.  
There is no black-box automation in reporting chains."*

### For ASIC:

*"No consumer-affecting outcome can ever be AI-originated in this model.  
All advice is filtered by human-approved policy and execution controls.  
Board accountability is explicit and continuous."*

---

## Slide 8 — What We Are Not Asking For (30 seconds)

**Say clearly:**

*"We are not seeking approval for AI execution today.*

*We are not seeking model waivers.*

*We are not asking to weaken human controls."*

**Then:**

*"We are asking whether this pre-execution governance posture meets supervisory expectations."*

---

## Slide 9 — Our Proposed Supervisory Engagement Model (60 seconds)

**Say:**

*"We see three phases:*

- *Phase 1 — Shadow only (current).*
- *Phase 2 — Advisory with mandatory human confirmation.*
- *Phase 3 — Execution only if ever separately approved."*

**Then:**

*"Each phase would be accompanied by an explicit supervisory review, not an implied progression."*

---

## Slide 10 — The Four Questions We Are Formally Putting to You (60 seconds)

**Read these verbatim:**

1. *"Is the concept of weekly, immutable AI governance artefacts acceptable as supervisory evidence?"*

2. *"Does our mechanical non-execution posture meet baseline expectations for AI containment?"*

3. *"Are our proposed escalation thresholds for Fraud, AML, and Treasury aligned with supervisory norms?"*

4. *"If advisory-only overrides were ever considered, what minimum guardrails would you expect before execution discussions?"*

---

## Slide 11 — Your Close (30 seconds)

**Say this exactly:**

*"We would prefer to be corrected at architecture stage rather than at enforcement stage.*

*Our objective is to build something that is boring from a compliance perspective and impressive only in its discipline."*

---

## How to Handle the Three Most Likely Regulator Questions

### 1. "What stops this from quietly becoming automated later?"

**Answer:**

*"The same thing that stops a person without a licence driving a heavy vehicle — the system physically lacks the credentials, routes, and runtime permissions to execute. Adding them would be a licensing and deployment event, not a configuration tweak."*

### 2. "How do we independently verify this?"

**Answer:**

*"Through object-locked regulator annexes, schema violation counters, AI-origin execution counters, and replay hashes. You do not have to trust us — the system testifies mechanically."*

### 3. "Is this really necessary at shadow stage?"

**Answer:**

*"History suggests most AI failures occur between proof-of-concept and first execution. We are choosing to make that gap visible rather than informal."*

---

## What Success Looks Like at the End of the Meeting

If the meeting goes well, the regulators will not say "approved".  
They will instead say things like:

- *"This is the right way to approach AI governance."*
- *"We would expect to see these artefacts continue."*
- *"Come back before advisory execution."*
- *"Keep producing this weekly evidence."*

**That is the correct outcome.**

---

**Document Version:** 1.0  
**Last Updated:** 08 Dec 2025  
**Next Review:** After first regulator pre-engagement meeting
